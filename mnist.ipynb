{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "javascript",
      "display_name": "Javascript"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNF3VjPjJHV"
      },
      "source": [
        "Run cell (commands) below and then hit F5 (refresh the page) right after that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkF16NwcMsZ"
      },
      "source": [
        "!npm install -g --unsafe-perm ijavascript\n",
        "!ijsinstall --install=global\n",
        "!jupyter-kernelspec list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMyXtMptltc9"
      },
      "source": [
        "# prepa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skqAs_UHc8_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e5daad-4425-4939-a7e3-fa1c73e4f27f"
      },
      "source": [
        "var { spawnSync } = require('child_process');\n",
        "var sh = (cmd) => { \n",
        "    let res = spawnSync(cmd, { cwd: process.cwd(), stdio: 'pipe', shell: true, encoding: 'utf-8' })\n",
        "    console.log(...res.output);\n",
        "};\n",
        "var run_async = async (pf) => {\n",
        "  $$.async();\n",
        "  await pf();\n",
        "  $$.done();\n",
        "};\n",
        "sh('npm init -y');\n",
        "sh('node -v; npm -v');"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "null Wrote to /content/package.json:\n",
            "\n",
            "{\n",
            "  \"name\": \"content\",\n",
            "  \"version\": \"1.0.0\",\n",
            "  \"main\": \"index.js\",\n",
            "  \"scripts\": {\n",
            "    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n",
            "  },\n",
            "  \"keywords\": [],\n",
            "  \"author\": \"\",\n",
            "  \"license\": \"ISC\",\n",
            "  \"dependencies\": {\n",
            "    \"@tensorflow/tfjs-node-gpu\": \"^3.6.1\",\n",
            "    \"csv\": \"^5.5.0\",\n",
            "    \"sharp\": \"^0.28.2\"\n",
            "  },\n",
            "  \"devDependencies\": {},\n",
            "  \"description\": \"\"\n",
            "}\n",
            "\n",
            "\n",
            " \n",
            "null v14.16.0\n",
            "6.14.8\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWGUzZ7Y7SFG",
        "outputId": "442a10f5-85fb-4df2-9b42-a6cd9d3a6ed5"
      },
      "source": [
        "{\n",
        "  let fs = require('fs')\n",
        "  fs.mkdirSync('/root/.kaggle/', { recursive: true })\n",
        "  fs.copyFileSync('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "  sh('kaggle competitions download -c digit-recognizer')\n",
        "  sh('unzip train.csv.zip')\n",
        "  sh('unzip test.csv.zip')\n",
        "  sh('npm install @tensorflow/tfjs-node-gpu')\n",
        "  sh('npm install sharp')\n",
        "  sh('npm install csv')\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "null Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            " \n",
            "null Archive:  train.csv.zip\n",
            " replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n",
            "\n",
            "null Archive:  test.csv.zip\n",
            " replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n",
            "\n",
            "null \n",
            "> @tensorflow/tfjs-node-gpu@3.6.1 install /content/node_modules/@tensorflow/tfjs-node-gpu\n",
            "> node scripts/install.js gpu download\n",
            "\n",
            "GPU-linux-3.6.1.tar.gz\n",
            "+ @tensorflow/tfjs-node-gpu@3.6.1\n",
            "updated 1 package and audited 168 packages in 17.049s\n",
            "\n",
            "6 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 0 vulnerabilities\n",
            "\n",
            " npm WARN deprecated node-pre-gyp@0.14.0: Please upgrade to @mapbox/node-pre-gyp: the non-scoped node-pre-gyp package is deprecated and only the @mapbox scoped package will recieve updates in the future\n",
            "* Downloading libtensorflow\n",
            "\n",
            "* Building TensorFlow Node.js bindings\n",
            "npm WARN content@1.0.0 No description\n",
            "npm WARN content@1.0.0 No repository field.\n",
            "\n",
            "\n",
            "null \n",
            "> sharp@0.28.2 install /content/node_modules/sharp\n",
            "> (node install/libvips && node install/dll-copy && prebuild-install) || (node-gyp rebuild && node install/dll-copy)\n",
            "\n",
            "sharp: Using cached /root/.npm/_libvips/libvips-8.10.6-linux-x64.tar.br\n",
            "+ sharp@0.28.2\n",
            "updated 1 package and audited 168 packages in 2.354s\n",
            "\n",
            "6 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 0 vulnerabilities\n",
            "\n",
            " npm WARN content@1.0.0 No description\n",
            "npm WARN content@1.0.0 No repository field.\n",
            "\n",
            "\n",
            "null + csv@5.5.0\n",
            "updated 1 package and audited 168 packages in 1.273s\n",
            "\n",
            "11 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 0 vulnerabilities\n",
            "\n",
            " npm WARN content@1.0.0 No description\n",
            "npm WARN content@1.0.0 No repository field.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SisKGrRM9kR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "eb075f1e-7c80-4ee1-cda1-d2b6c7ed0187"
      },
      "source": [
        "//var sharp = require('sharp')\n",
        "var tf = require('@tensorflow/tfjs-node-gpu');\n",
        "var csv = require('csv/lib/sync')\n",
        "var sharp = require('sharp/lib/index')\n",
        "\n",
        "$$.async();\n",
        "process.on('unhandledRejection', error => {\n",
        "  console.log(error);\n",
        "  $$.done();\n",
        "  process.exit(1)\n",
        " \n",
        "});\n",
        "(async () => {\n",
        "  let data = csv.parse(fs.readFileSync('./train.csv')).slice(1);\n",
        "  global._dataset = data.map(d => {\n",
        "    let out = new Float32Array(10)\n",
        "    out[parseInt(d[0])] = 1;\n",
        "\n",
        "    let inp = new Float32Array(d.slice(1).map(v=>parseInt(v)))\n",
        "    return [\n",
        "      tf.tensor2d(out, [1, 10]),\n",
        "      tf.tensor4d(inp,[1,28,28,1]).div(255)\n",
        "    ]\n",
        "  })\n",
        "  let test = csv.parse(fs.readFileSync('./test.csv')).slice(1);\n",
        "  global._datasetTest = test.map(d => {\n",
        "    let inp = new Float32Array(d.map(v => parseInt(v)))\n",
        "    return tf.tensor4d(inp, [1, 28, 28, 1]).div(255)\n",
        "  })\n",
        "  \n",
        "  $$.png(\n",
        "    Buffer.from(\n",
        "      await sharp(\n",
        "        Buffer.from(global._dataset[0][1].mul(255).dataSync()),\n",
        "        { raw: { width: 28, height: 28, channels: 1 } }).png().toBuffer()\n",
        "    ).toString(\"base64\"))\n",
        "  $$.done()\n",
        "})()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAACXBIWXMAAAPoAAAD6AG1e1JrAAABKUlEQVRIie3UMaqDQBSF4emEgBhBFBG34SoMaBcLtcwiUroQy0jWIFgkpAnEJIWFuANBbERRzvggs4Tc4hX5F/Bx7wwzjP36V+V5vq5rFEVkYlEU8zwDCMOQRjwej9M0ATidTpvNhkD0PG8cRwDP51OWZQLRtu2yLAG0bbvb7QhEx3Ferxc+7fd7AjEMQ845gK7rzuezqqrfioZhvN9vgaZpSjDjdrsVW3PO+773fZ8AtSxLnCPnnOa6NU17PB780+12kySJAM2yTIx5vV5pRE3T7vc7gGmaXNclEHVdz/McwDAMZA/8cDiIxYuioBGDIOj7HsDlcjFNk0BUFKVpGjGm53kEImMsiiIhAiD7g4MgWJYFwDzPSZLQoIyxqqrquo7jmEz8xb7oD0Z0wxzD5YlGAAAAAElFTkSuQmCC"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC2P2nKW3kZG"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdCylUUZRrbs"
      },
      "source": [
        "model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJPTwp79NXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827e6e41-13cb-43a1-d212-379fb1288ff6"
      },
      "source": [
        "var tf = require('@tensorflow/tfjs-node-gpu');\n",
        "var model = tf.sequential();\n",
        "model.add(tf.layers.dropout({rate:0.1, inputShape: [28,28, 1]}));\n",
        "model.add(tf.layers.conv2d({ filters: 16, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true}));\n",
        "model.add(tf.layers.leakyReLU());\n",
        "model.add(tf.layers.conv2d({ filters: 20, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\n",
        "model.add(tf.layers.leakyReLU());\n",
        "model.add(tf.layers.conv2d({ filters: 24, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\n",
        "model.add(tf.layers.leakyReLU());\n",
        "model.add(tf.layers.conv2d({ filters: 3, kernelSize: [5, 5], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\n",
        "model.add(tf.layers.leakyReLU());\n",
        "model.add(tf.layers.reshape({ targetShape: [6 * 6 * 3] }));\n",
        "model.add(tf.layers.dense({ units: 10, kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\n",
        "model.add(tf.layers.softmax());\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output shape              Param #   \n",
            "=================================================================\n",
            "dropout_Dropout3 (Dropout)   [null,28,28,1]            0         \n",
            "_________________________________________________________________\n",
            "conv2d_Conv2D9 (Conv2D)      [null,22,22,16]           800       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_LeakyReLU9 (Leak [null,22,22,16]           0         \n",
            "_________________________________________________________________\n",
            "conv2d_Conv2D10 (Conv2D)     [null,16,16,20]           15700     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_LeakyReLU10 (Lea [null,16,16,20]           0         \n",
            "_________________________________________________________________\n",
            "conv2d_Conv2D11 (Conv2D)     [null,10,10,24]           23544     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_LeakyReLU11 (Lea [null,10,10,24]           0         \n",
            "_________________________________________________________________\n",
            "conv2d_Conv2D12 (Conv2D)     [null,6,6,3]              1803      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_LeakyReLU12 (Lea [null,6,6,3]              0         \n",
            "_________________________________________________________________\n",
            "reshape_Reshape3 (Reshape)   [null,108]                0         \n",
            "_________________________________________________________________\n",
            "dense_Dense3 (Dense)         [null,10]                 1090      \n",
            "_________________________________________________________________\n",
            "softmax_Softmax3 (Softmax)   [null,10]                 0         \n",
            "=================================================================\n",
            "Total params: 42937\n",
            "Trainable params: 42937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yacP0dkQRj5E"
      },
      "source": [
        "remove old models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgpBUb0CPuel"
      },
      "source": [
        "\n",
        "fs.readdirSync('./models').forEach(fileName => {\n",
        "  fs.rmdirSync('./models/'+fileName, { recursive: true })\n",
        "})"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2XvafFRlqa"
      },
      "source": [
        "train and save models to \"models\" folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS0oPUR9MxpR",
        "outputId": "28986c04-556d-4bf3-cb9a-887bd79cc18e"
      },
      "source": [
        "var tf = require('@tensorflow/tfjs-node-gpu');\n",
        "\n",
        "var validK = 0.99\n",
        "var trainFiles = global._dataset.slice(0, Math.floor(global._dataset.length * validK))\n",
        "var validFiles = global._dataset.slice(Math.ceil(global._dataset.length * validK))\n",
        "\n",
        "global.getImages = (files, index, batchSize) => {\n",
        " \n",
        "    let input = [];\n",
        "  let output = [];\n",
        "    for (let i = 0; i !== batchSize; i++) {\n",
        "        input.push(files[index + i][1]);\n",
        "        output.push(files[index + i][0]);\n",
        "    }\n",
        "    return { input: tf.concat(input), output: tf.concat(output)}\n",
        "}\n",
        " \n",
        "global.getValidation = async (model) => {\n",
        "  let errorL = tf.scalar(0);\n",
        "  let hit = 0;\n",
        "    for (let i = 0; i !== validFiles.length; i++) {\n",
        "        errorL = errorL.add(tf.tidy(() => {\n",
        "        let { input, output } = global.getImages(validFiles, i, 1)\n",
        "        \n",
        "          let result = model.predict(input);\n",
        "             let data = result.dataSync();\n",
        "      let maxV = -Infinity;\n",
        "      let maxI = 0;\n",
        "      data.forEach((v,i) => {\n",
        "        if (v > maxV) {\n",
        "          maxV = v;\n",
        "          maxI = i;\n",
        "        }\n",
        "      })\n",
        "          if (output.dataSync()[maxI]||0===1) {\n",
        "            hit++;\n",
        "      }\n",
        "            return loss(result, output)\n",
        "        }))\n",
        "    }\n",
        "    return [errorL.div(validFiles.length).dataSync()[0],hit/validFiles.length];\n",
        "}\n",
        " \n",
        "global.minimize = (input, output) => {\n",
        "        optimizer.minimize(() => {\n",
        "          let result = model.predict(input);\n",
        "            \n",
        "            return loss(result, output)\n",
        "        })\n",
        "}\n",
        "global.optimizer = tf.train.adam(3.0e-4);\n",
        "global.loss = tf.losses.meanSquaredError;\n",
        " \n",
        "global.main = async () => {\n",
        "    let epochs = 50;\n",
        "  let batchSize = 512;\n",
        "    \n",
        "    \n",
        "    for (let e = 0; e !== epochs; e++) {\n",
        "     \n",
        "        let time = new Date().valueOf();\n",
        "        for (let i = 0; i !== Math.floor(trainFiles.length / batchSize) * batchSize; i += batchSize) {\n",
        " \n",
        "        tf.tidy(() => {\n",
        "            //console.log(i);\n",
        "            let { input, output } = global.getImages(trainFiles, i, batchSize);\n",
        "            minimize(input, output);\n",
        "\n",
        "        })\n",
        "        }\n",
        "         console.log(e, Date.now()- time)\n",
        "        let shuffle = (array) => array.sort(() => Math.random() - 0.5);\n",
        "        trainFiles = shuffle(trainFiles)\n",
        "        let valError = await global.getValidation(model)\n",
        "      console.log('valError: ', valError);\n",
        "         try {\n",
        "    await fs.promises.mkdir('models/');\n",
        "  }catch(err){}\n",
        "        model.save(`file://./models/${e}_${valError[0]}`)\n",
        "    }\n",
        "\n",
        "\n",
        " \n",
        "}\n",
        "global.main();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 9492\n",
            "valError:  [ 0.0035864408127963543, 0.9761904761904762 ]\n",
            "1 9174\n",
            "valError:  [ 0.0017407124396413565, 0.9928571428571429 ]\n",
            "2 9283\n",
            "valError:  [ 0.003185026813298464, 0.9761904761904762 ]\n",
            "3 9119\n",
            "valError:  [ 0.0018053409876301885, 0.9880952380952381 ]\n",
            "4 9299\n",
            "valError:  [ 0.0037724534049630165, 0.9785714285714285 ]\n",
            "5 9079\n",
            "valError:  [ 0.002384751569479704, 0.9880952380952381 ]\n",
            "6 9114\n",
            "valError:  [ 0.002425662474706769, 0.9833333333333333 ]\n",
            "7 9601\n",
            "valError:  [ 0.002873930148780346, 0.9857142857142858 ]\n",
            "8 9177\n",
            "valError:  [ 0.0024221199564635754, 0.9833333333333333 ]\n",
            "9 9068\n",
            "valError:  [ 0.001903802971355617, 0.9880952380952381 ]\n",
            "10 9205\n",
            "valError:  [ 0.003074571955949068, 0.9761904761904762 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoPFyNELRgE8"
      },
      "source": [
        "Test best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrjyQJL0Oino",
        "outputId": "d73f9dfa-0882-4f26-b246-aa8cfbdf9007"
      },
      "source": [
        "(async () => {\n",
        "  //get best model\n",
        "  let testModel = await tf.loadLayersModel('file://./models/' + fs.readdirSync('./models').sort((l, r) => parseFloat(l.split('_')[1]) - parseFloat(r.split('_')[1]))[0] + '/model.json');\n",
        "  let valError = await global.getValidation(testModel)\n",
        "  console.log(valError)\n",
        "})()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0016014677239581943, 0.9952380952380953 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onLRCFyZRYL4"
      },
      "source": [
        "Generate csv file for competition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytoZ9P8KCJr9"
      },
      "source": [
        "(async () => {\n",
        "  //get best model\n",
        "  let testModel = await tf.loadLayersModel('file://./models/' + fs.readdirSync('./models').sort((l, r) => parseFloat(l.split('_')[1]) - parseFloat(r.split('_')[1]))[0] + '/model.json');\n",
        "\n",
        "  let out =[['ImageId','Label']]\n",
        "  global._datasetTest.forEach((v, ImageId) => {\n",
        "    tf.tidy(() => {\n",
        "\n",
        "      let data = testModel.predict(v).dataSync();\n",
        "      let maxV = -Infinity;\n",
        "      let maxI = 0;\n",
        "      data.forEach((v,i) => {\n",
        "        if (v > maxV) {\n",
        "          maxV = v;\n",
        "          maxI = i;\n",
        "        }\n",
        "      })\n",
        "      out.push([String(ImageId+1),String(maxI)])\n",
        "\n",
        "     })\n",
        "   })\n",
        "  fs.writeFileSync('out.csv',csv.stringify(out))\n",
        "\n",
        "})()"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4derYjJCboQo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bgi5_6vNN6u",
        "outputId": "36889833-e54d-4a67-9bb4-240fc5c008d9"
      },
      "source": [
        "sh(`kaggle competitions submit -c digit-recognizer -f out.csv -m \"tfjs\"`)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "null Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Successfully submitted to Digit Recognizer \r  0%|          | 0.00/208k [00:00<?, ?B/s]\r100%|##########| 208k/208k [00:08<00:00, 24.2kB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}